# .env

# Model ID to load in vLLM
MODEL_ID=codellama/CodeLlama-13b-Instruct-hf

# Number of GPUs to use for tensor parallelism (adjust based on your setup)
TP_SIZE=1

# Maximum length for the model (adjust based on your needs and GPU memory)
MAX_LEN=8192